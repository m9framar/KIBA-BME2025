{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"blk1804/kiba-drug-binding-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming the CSV file is named 'data.csv' in the provided path\n",
    "csv_path = f\"{path}/KIBA.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the last column to 'interaction'\n",
    "df.rename(columns={'Ki , Kd and IC50  (KIBA Score)': 'interaction'}, inplace=True)\n",
    "df.rename(columns={'compound_iso_smiles': 'smiles'}, inplace=True)\n",
    "\n",
    "# Show the distribution of the 'interaction' column\n",
    "interaction_distribution = df['interaction'].describe()\n",
    "print(interaction_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import MACCSkeys\n",
    "from rdkit import DataStructs\n",
    "import numpy as np\n",
    "\n",
    "def smiles_to_maccs_fp(smiles_list):\n",
    "    fingerprints = []\n",
    "    \n",
    "    # MACCS keys produce a 167-bit fingerprint.\n",
    "    for smiles in smiles_list:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is not None:\n",
    "            fp = MACCSkeys.GenMACCSKeys(mol)\n",
    "            arr = np.zeros((167,), dtype=np.int8)\n",
    "            DataStructs.ConvertToNumpyArray(fp, arr)\n",
    "            fingerprints.append(arr)\n",
    "        else:\n",
    "            fingerprints.append(np.zeros(167, dtype=np.int8))\n",
    "    \n",
    "    return np.array(fingerprints)\n",
    "def process_smiles_in_batches(smiles_list, batch_size=1000):\n",
    "    all_fingerprints = []\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in range(0, len(smiles_list), batch_size):\n",
    "        batch = smiles_list[i:i+batch_size]\n",
    "        batch_fps = smiles_to_maccs_fp(batch)\n",
    "        all_fingerprints.append(batch_fps)\n",
    "        \n",
    "        \n",
    "    # Combine results\n",
    "    return np.vstack(all_fingerprints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def protein_feature_hashing(protein_sequences, n_features=1000, batch_size=1000):\n",
    "    from sklearn.feature_extraction.text import HashingVectorizer\n",
    "    \n",
    "    all_encodings = []\n",
    "    vectorizer = HashingVectorizer(n_features=n_features, analyzer='char', ngram_range=(3, 3))\n",
    "    \n",
    "    for i in range(0, len(protein_sequences), batch_size):\n",
    "        batch = protein_sequences[i:i+batch_size]\n",
    "        encodings = vectorizer.transform(batch).toarray()\n",
    "        all_encodings.append(encodings)\n",
    "        \n",
    "    \n",
    "    return np.vstack(all_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concordance_index(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes the Concordance Index (CI) as used in DeepDTA.\n",
    "    For each pair of samples with distinct true values, we check if the prediction ordering matches the true ordering.\n",
    "    Ties in prediction differences count as 0.5.\n",
    "    \"\"\"\n",
    "    n = 0  # Total number of comparable pairs\n",
    "    n_correct = 0  # Count of correctly ordered pairs\n",
    "    for i in range(len(y_true)):\n",
    "        for j in range(i + 1, len(y_true)):\n",
    "            if y_true[i] != y_true[j]:\n",
    "                n += 1\n",
    "                diff_true = y_true[i] - y_true[j]\n",
    "                diff_pred = y_pred[i] - y_pred[j]\n",
    "                if diff_true * diff_pred > 0:\n",
    "                    n_correct += 1\n",
    "                elif diff_pred == 0:\n",
    "                    n_correct += 0.5\n",
    "    return n_correct / n if n > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Combine feature sets\n",
    "X_compounds = process_smiles_in_batches(df['smiles'])\n",
    "print(\"compounds encoded\")\n",
    "X_proteins = protein_feature_hashing(df['target_sequence'])\n",
    "print(\"proteins encoded\")\n",
    "\n",
    "# You can concatenate the features\n",
    "X = np.hstack((X_compounds, X_proteins))\n",
    "y = df['interaction']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "model = RandomForestRegressor(n_estimators=100, n_jobs=-1, random_state=42)\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.27286430854288973\n",
      "R2: 0.6043449036109865\n",
      "Concordance Index: 0.8229810116975014\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "print(f\"MSE: {mean_squared_error(y_test, y_pred)}\")\n",
    "print(f\"R2: {r2_score(y_test, y_pred)}\")\n",
    "from lifelines.utils import concordance_index\n",
    "ci_value = concordance_index(y_test, y_pred)\n",
    "print(\"Concordance Index:\", ci_value)\n",
    "\n",
    "# Feature importance for interpretability\n",
    "feature_importances = model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def plot_top_features(model, n_features=20, figsize=(12, 8)):\n",
    "    \"\"\"\n",
    "    Plot the top n_features by importance from a model.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : model with feature_importances_ attribute\n",
    "        Trained model like RandomForest, GradientBoosting, etc.\n",
    "    n_features : int\n",
    "        Number of top features to show\n",
    "    figsize : tuple\n",
    "        Figure size\n",
    "    \"\"\"\n",
    "    # Get feature importances\n",
    "    importances = model.feature_importances_\n",
    "    \n",
    "    # Create indices array for features\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    \n",
    "    # Take the top n features\n",
    "    top_indices = indices[:n_features]\n",
    "    top_importances = importances[top_indices]\n",
    "    \n",
    "    # Create labels for MACCS keys\n",
    "    # MACCS keys are numbered 1-166 in RDKit, but we use 0-indexed arrays\n",
    "    labels = [f\"MACCS Key {idx+1}\" for idx in top_indices]\n",
    "    \n",
    "    # Create DataFrame for easier plotting\n",
    "    df_plot = pd.DataFrame({\n",
    "        'Feature': labels,\n",
    "        'Importance': top_importances\n",
    "    }).sort_values('Importance')\n",
    "    \n",
    "    # Create horizontal bar chart\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.barh(df_plot['Feature'], df_plot['Importance'], color='skyblue')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.title(f'Top {n_features} MACCS Keys by Importance')\n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.6)\n",
    "    \n",
    "    # Add importance values as text\n",
    "    for i, v in enumerate(df_plot['Importance']):\n",
    "        plt.text(v + 0.001, i, f'{v:.4f}', va='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Return the top indices and their importances\n",
    "    return pd.DataFrame({\n",
    "        'MACCS Key Index': [idx+1 for idx in top_indices],  # 1-indexed for MACCS keys\n",
    "        'Importance': top_importances\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "plot_top_features(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

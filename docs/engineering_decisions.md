# Engineering Decisions for KIBA GPU/LLM Implementation

## Initial Setup (April 22, 2025)

### 1. Framework Choice: PyTorch
- **Rationale:** PyTorch is chosen as the primary deep learning framework due to its strong community support, extensive documentation, ease of use for research and development, and excellent integration with Hugging Face's `transformers` library. Its dynamic computation graph is beneficial for handling variable-length sequences (like SMILES and protein sequences). Native GPU acceleration is well-supported.

### 2. Potential LLMs for Embeddings
- **SMILES (Drugs):** Models like ChemBERTa (e.g., `seyonec/ChemBERTa-zinc-base-v1`) pre-trained on large chemical datasets are strong candidates. They understand chemical syntax and structure.
- **Protein Sequences:** Models like ESM (Evolutionary Scale Modeling, e.g., `facebook/esm2_t6_8M_UR50D`) or ProtBERT (e.g., `Rostlab/prot_bert`) pre-trained on vast protein sequence databases capture biological context. The specific model size (e.g., 8M parameters for ESM2) will be chosen based on initial performance vs. resource trade-offs. Smaller models allow faster iteration.
- **Consideration:** We will need to evaluate how to best pool or utilize the token embeddings generated by these models (e.g., mean pooling, CLS token) to get a fixed-size representation for each molecule/protein.

### 3. Containerization: Docker with NVIDIA CUDA Base Image
- **Rationale:** To ensure reproducibility and handle the complexities of GPU dependencies (CUDA, cuDNN), Docker is essential.
- **Base Image:** `nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu22.04` is selected. The CUDA version (11.8) matches the PyTorch build specified in `requirements.txt` (`--index-url https://download.pytorch.org/whl/cu118`). Using a runtime image keeps the size smaller than a devel image. Ubuntu 22.04 provides a modern base OS. Python 3.12 is used based on project context.
- **Dependencies:** Python 3.12 is installed along with dependencies via `pip` and `requirements.txt`.

### 4. Code Structure
- **Separation:** The new implementation will reside in the `src/` directory, separate from the initial `KIBA.ipynb` notebook, promoting modularity.
- **Main Script:** `src/main.py` will serve as the entry point.
- **Documentation:** This file (`docs/engineering_decisions.md`) will track key decisions.

## Implementation Phase 1 (April 22, 2025)

### 1. Configuration: `argparse`
- **Rationale:** Using Python's built-in `argparse` module allows for easy configuration of hyperparameters (like model names, batch size, learning rate, epochs) and operational parameters (like output directory) via command-line arguments. This makes the script flexible and suitable for running experiments within the Docker container or on a server. Default values are provided for ease of use.

### 2. Data Loading: `kagglehub` and `pandas`
- **Rationale:** Leveraging `kagglehub` ensures the dataset is easily downloadable and accessible within different environments (local, Docker). `pandas` is used for initial data manipulation (loading CSV, renaming columns, basic cleaning like dropping NA values), consistent with the original notebook.

### 3. LLM Loading: Hugging Face `transformers`
- **Rationale:** The `transformers` library provides a standardized way (`AutoTokenizer`, `AutoModel`) to load a wide variety of pre-trained models and their corresponding tokenizers using just the model name (e.g., "seyonec/ChemBERTa-zinc-base-v1"). This simplifies the process significantly. Models are loaded onto the appropriate device (GPU if available, otherwise CPU).

### 4. Initial Embedding Strategy: Placeholder Functions
- **Rationale:** Full implementation of the embedding generation (tokenization, passing through the model, pooling token embeddings) requires careful handling of batching, padding, and device placement. Placeholder functions (`get_smiles_embeddings`, `get_protein_embeddings`) returning random tensors of the correct expected shape are implemented first. This allows the overall structure (data loading, splitting, placeholder for training) to be built and tested before tackling the complexity of the actual embedding generation. Models are set to `eval()` mode and `torch.no_grad()` is used, as gradients are not needed for generating fixed embeddings.

### 5. Logging: Python `logging` Module
- **Rationale:** Using the standard `logging` module provides informative output about the script's progress (data loading, model loading, device usage, warnings) and is more robust than simple `print` statements, especially for tracking execution within Docker.

## Implementation Phase 2 (April 22, 2025)

### 1. Data Handling: PyTorch `Dataset` and `DataLoader`
- **Rationale:** Using PyTorch's `Dataset` class (`KIBADataset`) provides a standard way to wrap the pre-computed embeddings and target values. The `DataLoader` then handles efficient batching, shuffling (for training), and parallel data loading (`num_workers`), which is crucial for performance, especially when the GPU is waiting for data.
- **Implementation:** The `KIBADataset` concatenates the SMILES and protein embeddings for each pair. `DataLoader` is configured with `pin_memory=True` which can speed up CPU-to-GPU data transfers.

### 2. Regression Model: Multi-Layer Perceptron (MLP)
- **Rationale:** As a starting point, a simple MLP (`RegressionMLP`) is implemented. It takes the concatenated embeddings as input and passes them through a few fully connected layers with ReLU activations and Dropout for regularization. This provides a baseline deep learning model to compare against the original RandomForest and potential future, more complex architectures.
- **Architecture:** Input Layer -> Linear(input_dim, 512) -> ReLU -> Dropout(0.2) -> Linear(512, 256) -> ReLU -> Dropout(0.2) -> Linear(256, 1) -> Output.

### 3. Training Loop (`train_epoch`)
- **Rationale:** A standard PyTorch training loop is implemented.
- **Components:**
    - Sets model to `train()` mode.
    - Iterates through batches from `DataLoader`.
    - Moves data to the target `device`.
    - Performs forward pass, loss calculation (MSE), backward pass (`loss.backward()`), and optimizer step (`optimizer.step()`).
    - Uses `Adam` optimizer, a common and effective choice.
    - Includes `tqdm` for a progress bar.
    - Logs average epoch loss and debug-level batch loss.

### 4. Evaluation Loop (`evaluate`)
- **Rationale:** A separate evaluation loop ensures consistent evaluation on the test set after each epoch.
- **Components:**
    - Sets model to `eval()` mode and uses `torch.no_grad()`.
    - Iterates through batches from the test `DataLoader`.
    - Calculates loss (MSE).
    - Collects all predictions and targets.
    - Calculates final metrics: MSE, RMSE, R² using `sklearn.metrics`.
    - Logs evaluation metrics.
    - **Note:** Concordance Index (CI) is mentioned as a TODO, as it was used in the original notebook and is a common metric for this task. It requires a slightly different calculation than MSE/R².

### 5. Model Checkpointing
- **Rationale:** To save the best performing model during training, the state dictionary (`model.state_dict()`) is saved whenever the validation loss decreases. This prevents losing progress and allows reloading the best model later.
- **Implementation:** Saves the best model to `best_model.pth` in the output directory.

### 6. Result Presentation & Persistence
- **Rationale:** To improve upon simple print statements, the final evaluation metrics, metrics tracked per epoch, and the configuration arguments used for the run are saved into a structured JSON file (`final_metrics.json`).
- **Implementation:** Uses the `json` library to dump the results dictionary into the specified output directory. This makes it easy to compare different runs programmatically or manually.